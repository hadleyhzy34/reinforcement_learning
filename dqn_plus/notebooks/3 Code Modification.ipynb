{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. Double DQN 代码修改\n",
    "\n",
    "2. Duel DQN 代码修改\n",
    "\n",
    "3. 基础Replay Buffer 代码修改\n",
    "\n",
    "4. Prioritized Experience Replay 代码修改\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Double DQN 代码修改\n",
    "\n",
    "根据论文我们知道，Double DQN只改动了对于target value的计算，而我们对于target value的计算只在agent.learn()中进行，所以也只需要进行少量的修改即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    def learn(self):\n",
    "        states, actions, rewards, next_states, dones = random.sample(self.memory, self.bs)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "    \n",
    "        Q_values = self.Q_local(states)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            ############################## Change Here ##############################\n",
    "            if not self.double: # this is a True/False parameter when initialize this agent\n",
    "                Q_targets, _ = torch.max(input=Q_targets, dim=1, keepdim=True)\n",
    "            else:\n",
    "                inner_actions = torch.max(input=self.Q_local(next_states), dim=1, keepdim=True)[1]\n",
    "                Q_targets = torch.gather(input=Q_targets, dim=1, index=inner_actions)\n",
    "            ############################## Change Ends ##############################\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "    \n",
    "        deltas = Q_values - Q_targets\n",
    "        loss = deltas.pow(2).mean()\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Duel DQN 代码修改\n",
    "\n",
    "Duel Network Structure只是网络结构的更改，所以需要的更改的只有网络结构文件network.py，在agent中只需要加入对应的参数即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64], duel=False):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.duel = duel\n",
    "        if self.duel:\n",
    "            self.fc4 = nn.Linear(hidden[1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.duel:\n",
    "            x1 = self.fc3(x)\n",
    "            x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0] # set the max to be 0\n",
    "            x2 = self.fc4(x)\n",
    "            return x1 + x2\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 基础 Replay Buffer 代码修改\n",
    "\n",
    "在原先的代码中，agent使用一个deque作为replay buffer，所以这里需要的修改是Replay Buffer的初始化，以及transition的进入和抽样。核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In agent.py\n",
    "\n",
    "class agent:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.memory = Replay_Buffer(int(1e5), bs)\n",
    "    \n",
    "    def learn():\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.bs)\n",
    "        \n",
    "# Do not require other changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prioritized Experience Replay 代码修改\n",
    "\n",
    "在Prioritized Experience Replay中，主要的部分都可以在Replay Buffer内部进行，这些我们已经实现了，剩下的必须在训练过程中完成的部分是：\n",
    "  * 计算权重$w_i$\n",
    "  * 更新TD-error $\\delta_i$\n",
    "  * 将error返回到Replay Buffer中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ############################## Change Here ##############################\n",
    "        if not self.prioritized:\n",
    "            self.memory = Replay_Buffer(int(1e5), bs)\n",
    "        else:\n",
    "            #self.memory = Rank_Replay_Buffer(int(1e5), bs)\n",
    "            # or\n",
    "            self.memory = Proportion_Replay_Buffer(int(1e5), bs)\n",
    "        ############################## Change Ends ##############################\n",
    "\n",
    "    def learn(self):\n",
    "        ############################## Change Here ##############################\n",
    "        if not self.prioritized:\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample(self.bs)\n",
    "            w = torch.ones(actions.size())\n",
    "            w = w.to(self.device)\n",
    "        else:\n",
    "            index_set, states, actions, rewards, next_states, dones, probs = self.memory.sample(self.bs)\n",
    "            w = 1/len(self.memory)/probs\n",
    "            w = w/torch.max(w)\n",
    "            w = w.to(self.device)\n",
    "        ############################## Change Ends ##############################\n",
    "            \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        Q_values = self.Q_local(states)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "\n",
    "        deltas = Q_values - Q_targets\n",
    "        ############################## Change Here ##############################\n",
    "        loss = (w*deltas.pow(2)).mean()\n",
    "        ############################## Change Ends ##############################\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        ############################## Change Here ##############################\n",
    "        if self.prioritized:\n",
    "            deltas = np.abs(deltas.detach().cpu().numpy().reshape(-1))\n",
    "            for i in range(self.bs):\n",
    "                self.memory.insert(deltas[i], index_set[i])\n",
    "        ############################## Change Ends ##############################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
